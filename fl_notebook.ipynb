{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebagdasa/federated_homework/blob/master/fl_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb2a795e",
      "metadata": {
        "id": "fb2a795e"
      },
      "source": [
        "# Federated Learning with PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3552c15b",
      "metadata": {
        "id": "3552c15b"
      },
      "source": [
        "Your task is to train a model that recognizes [MNIST digits](https://en.wikipedia.org/wiki/MNIST_database) using Federated Learning that creates a joint global model from multiple local models trained on user data. Please consult the [original paper](https://arxiv.org/abs/1602.05629) and don't hesitate to ask questions.\n",
        "\n",
        "We provide some skeleton code and a reference implementation of the centralized training from [PyTorch examples](https://github.com/pytorch/examples/blob/master/mnist/main.py).\n",
        "\n",
        "**Your task:** fill up the skeleton code and write a training procedure for FL. \n",
        "\n",
        "Submit this notebook to Canvas and we will run it and examine the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0fa404",
      "metadata": {
        "id": "fe0fa404"
      },
      "source": [
        "We slightly modify the main algorithm at the paper in the following way:\n",
        "\n",
        "- We compute a local update, i.e. difference between local and global updates: L-G<sup>t</sup>.\n",
        "- Accumilated sum of local updates is divided by a number of users in this round `round_size`.\n",
        "- The global model at next round G<sup>t+1</sup>= G<sup>t</sup> + `global_lr` * Sum(L<sub>i</sub>-G<sup>t</sup>)/`round_size`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb79f31",
      "metadata": {
        "id": "3eb79f31"
      },
      "source": [
        "## Helper functions and parameters\n",
        "\n",
        "No need to modify this part but you need to be familiar with the primitives: data loaders, models, central training and testing functions. It's mainly taken from [PyTorch examples](https://github.com/pytorch/examples/blob/main/mnist/main.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "330e0630",
      "metadata": {
        "id": "330e0630"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import Parameter\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.optim import SGD\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import List, Dict, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "861da32c",
      "metadata": {
        "id": "861da32c"
      },
      "outputs": [],
      "source": [
        "# use cpu or cuda\n",
        "use_cuda=True\n",
        "\n",
        "# Learning rate for the update of global model\n",
        "global_lr = 0.1\n",
        "local_lr = 0.1\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# FL parameters\n",
        "no_users = 100\n",
        "no_rounds = 5\n",
        "round_size = 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b84acbb",
      "metadata": {
        "id": "4b84acbb"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "94d06894",
      "metadata": {
        "id": "94d06894",
        "outputId": "04ca5259-3ea5-44c5-a373-fc0b45b0ce6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 165459662.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 96599436.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 36532483.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6564620.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_kwargs = {'batch_size': batch_size}\n",
        "test_kwargs = {'batch_size': batch_size}\n",
        "if use_cuda:\n",
        "    device='cuda'\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                   'pin_memory': True,\n",
        "                   }\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "else:\n",
        "    device='cpu'\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
        "                   transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85a928e5",
      "metadata": {
        "id": "85a928e5"
      },
      "source": [
        "### Split training dataset into smaller ones for each participant\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "86353687",
      "metadata": {
        "id": "86353687"
      },
      "outputs": [],
      "source": [
        "def split_data(training_dataset, no_users):\n",
        "    data_per_user = len(training_dataset) // no_users\n",
        "    data_loaders = list()\n",
        "    for i in range(no_users):\n",
        "        indices = random.sample(list(range(len(training_dataset))), data_per_user)\n",
        "        sampler = torch.utils.data.SubsetRandomSampler(indices)\n",
        "        data_loader = torch.utils.data.DataLoader(training_dataset, sampler=sampler, **train_kwargs)\n",
        "        data_loaders.append(data_loader)\n",
        "        \n",
        "    return data_loaders\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "05b5c1e0",
      "metadata": {
        "id": "05b5c1e0"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transform)\n",
        "\n",
        "data_loaders = split_data(train_dataset, no_users)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed47337",
      "metadata": {
        "id": "8ed47337"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "37fcd530",
      "metadata": {
        "id": "37fcd530"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "125f3d75",
      "metadata": {
        "id": "125f3d75"
      },
      "source": [
        "### Testing of the global model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "169db479",
      "metadata": {
        "id": "169db479"
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader):\n",
        "    \"\"\"Perform testing of the global aggregated model.\n",
        "    \n",
        "    Args:\n",
        "        model: torch.nn global model.\n",
        "        train_loader: loader for global testing data.\n",
        "    \n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "        \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebd6a2f2",
      "metadata": {
        "id": "ebd6a2f2"
      },
      "source": [
        "### Example of a central training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8a3d9bf2",
      "metadata": {
        "id": "8a3d9bf2"
      },
      "outputs": [],
      "source": [
        "def train(epoch: int, model: Net, train_loader: DataLoader, optimizer: optim.Optimizer):\n",
        "    \"\"\" Centralized training.\n",
        "    \n",
        "    Args:\n",
        "        epoch: training epoch.\n",
        "        model: torch.nn model.\n",
        "        train_loader: loader for global training data.\n",
        "        optimizer: optimizer for global model.\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f32920",
      "metadata": {
        "id": "84f32920"
      },
      "source": [
        "### Here are some ideas to help you navigate PyTorch training for FL:\n",
        "\n",
        "1. No need to do parallel training of the models in one round. You can sequentially train each model in a round and accumulate the model weights of trained models.\n",
        "\n",
        "2. Once the result in a round is summed up you can average it and apply to the global model.\n",
        "\n",
        "3. Be careful on copying tensors: participant's model before training needs to be a copy of the global model. It's useful to always keep `global_model` and create a `local_model` for every participant (use `deepcopy()`). The optimizer needs to be recreated for every `local_model`.\n",
        "\n",
        "**Primitives**:\n",
        "\n",
        "`model.state_dict()` -- returns a dictionary of layer names and parameters (weights) for the model. \n",
        "\n",
        "`param.data.detach()` -- useful for computing a difference between the global and local models. \n",
        "\n",
        "`param.data.add_(data)` -- modify the value of the weight tensor by adding `data` (useful when updating `global_model`).\n",
        "\n",
        "`copy.deepcopy(global_model)` -- copy the whole model (useful to create a `local_model`).\n",
        "\n",
        "`optimizer = optim.SGD(local_model.parameters(), lr=local_lr)` -- create optimizer for local model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc9c0b0",
      "metadata": {
        "id": "acc9c0b0"
      },
      "source": [
        "# Task begins"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5083c819",
      "metadata": {
        "id": "5083c819"
      },
      "source": [
        "## 1. Fill FL primitives\n",
        "\n",
        "Create a `local_train` function to train the model locally, use `accumulate` to sum local models into one object, and `average` to average these models and update the global model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7f0159a1",
      "metadata": {
        "id": "7f0159a1"
      },
      "outputs": [],
      "source": [
        "def local_train(model_id: int, global_model: Net, train_loader: DataLoader, \n",
        "                local_lr: float, clipping_norm: float=None) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Perform training of the local model on local data.\n",
        "    \n",
        "    Args:\n",
        "        model_id: identificator of the local model.\n",
        "        global_model: global model (cannot be modified!).\n",
        "        train_loader: loader for local training data.\n",
        "        local_lr: learning rate for the local optimizer.\n",
        "        clipping_norm: bound of model update for DP training.\n",
        "    \n",
        "    Returns:\n",
        "        Model update, i.e. for each param local_model - global_model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE GOES HERE (Hint: modify centralized training function)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a7b230d8",
      "metadata": {
        "id": "a7b230d8"
      },
      "outputs": [],
      "source": [
        "def accumulate(local_update: Dict[str, torch.Tensor], \n",
        "               weight_aggregator: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Accumulate local updates into a weight_aggregator.\n",
        "    \n",
        "    Args:\n",
        "        local_update: dictionary with updates. \n",
        "        weight_aggregator: sum of all local models from the single round.\n",
        "    \n",
        "    Returns: \n",
        "        Updated weight aggregator.\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fc0f51f8",
      "metadata": {
        "id": "fc0f51f8"
      },
      "outputs": [],
      "source": [
        "def average(no_models: int, global_model: Net, weight_aggregator: Dict[str, torch.Tensor], \n",
        "            global_lr: float, noise_std: float=None) -> Net:\n",
        "    \"\"\"Average accumulated models and apply them to the global model.\n",
        "    \n",
        "    Args:\n",
        "        global_model: Server's FL model\n",
        "        no_models: number of models in a single FL round.\n",
        "        weight_aggregator: sum of all local models from the single round.\n",
        "        global_lr: learning rate to update the global model.\n",
        "        noise_std: added noise for DP training.\n",
        "    \n",
        "    Returns:\n",
        "        Updated global model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0964540f",
      "metadata": {
        "id": "0964540f"
      },
      "source": [
        "## 2. Run FL training and testing\n",
        "\n",
        "\n",
        "Using above primitives implement Federated Learning routine by training \n",
        "a global model for `no_rounds` and sampling `round_size` users from `no_user` for each round.\n",
        "\n",
        "Don't forget to test the `global_model` on convergence. Successfully trained global model will score above 75% on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0cbfd1c2",
      "metadata": {
        "id": "0cbfd1c2"
      },
      "outputs": [],
      "source": [
        "global_model = Net().to(device)\n",
        "weight_aggregator = dict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcahtAC9Zh8Y"
      },
      "id": "kcahtAC9Zh8Y",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49af2730",
      "metadata": {
        "id": "49af2730"
      },
      "outputs": [],
      "source": [
        "# Training code goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd2405f",
      "metadata": {
        "id": "ccd2405f"
      },
      "source": [
        "## 3. Implement Differentially Private FL\n",
        "\n",
        "It's possible to protect user model updates by applying Differential Privacy (can check this [Google paper](https://arxiv.org/abs/1710.06963), Algorithm 1). Augment above FL code to support DP training by clipping each layer of the local model update by value **S** to limit update sensitivity and add Gaussian noise with std **sigma** when updating the global model (i.e. to the averaged update before scaling by global_lr).\n",
        "\n",
        "Modify **`local_training`** method to use `clipping_norm` and an **`aggregate`** method to add Gaussian noise with `noise_std`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dedd83e",
      "metadata": {
        "id": "4dedd83e"
      },
      "outputs": [],
      "source": [
        "# Your model accuracy should be higher than 60%\n",
        "\n",
        "S = 1\n",
        "sigma = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ccf5d1",
      "metadata": {
        "id": "b9ccf5d1"
      },
      "outputs": [],
      "source": [
        "# DP Training code goes here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c082a283",
      "metadata": {
        "id": "c082a283"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}